{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example GDD 📼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import psycopg2\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import connect_db, get_dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 147287\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>sentid</th>\n",
       "      <th>wordidx</th>\n",
       "      <th>words</th>\n",
       "      <th>poses</th>\n",
       "      <th>ners</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5705014ccf58f18a4c0d6d61</td>\n",
       "      <td>1</td>\n",
       "      <td>[1, 2, 3, 4]</td>\n",
       "      <td>[Eos, ,, Vol, .]</td>\n",
       "      <td>[NNS, ,, NNP, .]</td>\n",
       "      <td>[O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5705014ccf58f18a4c0d6d61</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[94, ,, No., 10, ,, 5, March, 2013, PAGE, 104,...</td>\n",
       "      <td>[CD, ,, NN, CD, ,, CD, NNP, CD, NN, CD, JJ, JJ...</td>\n",
       "      <td>[NUMBER, O, O, NUMBER, O, DATE, DATE, DATE, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5705014ccf58f18a4c0d6d61</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[Most, studies, of, solute, transport, through...</td>\n",
       "      <td>[JJS, NNS, IN, JJ, NN, IN, NNS, VBP, VBN, IN, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5705014ccf58f18a4c0d6d61</td>\n",
       "      <td>4</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[However, ,, small, impoundments, ,, such, as,...</td>\n",
       "      <td>[RB, ,, JJ, NNS, ,, JJ, IN, DT, VBN, IN, NN, C...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5705014ccf58f18a4c0d6d61</td>\n",
       "      <td>5</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>[As, these, small, systems, mature, ,, the, im...</td>\n",
       "      <td>[IN, DT, JJ, NNS, VBP, ,, DT, NNS, VBP, IN, NN...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      docid  sentid  \\\n",
       "0  5705014ccf58f18a4c0d6d61       1   \n",
       "1  5705014ccf58f18a4c0d6d61       2   \n",
       "2  5705014ccf58f18a4c0d6d61       3   \n",
       "3  5705014ccf58f18a4c0d6d61       4   \n",
       "4  5705014ccf58f18a4c0d6d61       5   \n",
       "\n",
       "                                             wordidx  \\\n",
       "0                                       [1, 2, 3, 4]   \n",
       "1  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "2  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "3  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "4  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...   \n",
       "\n",
       "                                               words  \\\n",
       "0                                   [Eos, ,, Vol, .]   \n",
       "1  [94, ,, No., 10, ,, 5, March, 2013, PAGE, 104,...   \n",
       "2  [Most, studies, of, solute, transport, through...   \n",
       "3  [However, ,, small, impoundments, ,, such, as,...   \n",
       "4  [As, these, small, systems, mature, ,, the, im...   \n",
       "\n",
       "                                               poses  \\\n",
       "0                                   [NNS, ,, NNP, .]   \n",
       "1  [CD, ,, NN, CD, ,, CD, NNP, CD, NN, CD, JJ, JJ...   \n",
       "2  [JJS, NNS, IN, JJ, NN, IN, NNS, VBP, VBN, IN, ...   \n",
       "3  [RB, ,, JJ, NNS, ,, JJ, IN, DT, VBN, IN, NN, C...   \n",
       "4  [IN, DT, JJ, NNS, VBP, ,, DT, NNS, VBP, IN, NN...   \n",
       "\n",
       "                                                ners  \n",
       "0                                       [O, O, O, O]  \n",
       "1  [NUMBER, O, O, NUMBER, O, DATE, DATE, DATE, O,...  \n",
       "2      [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]  \n",
       "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get dam names\n",
    "dams = get_dams()\n",
    "\n",
    "# Database connection\n",
    "df = connect_db()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_sent(sent): return ' '.join(sent)\n",
    "\n",
    "\n",
    "def n_sents(idx, df):\n",
    "    ''' Returns the surrounding sentences in rel to dataframe'''\n",
    "    start = idx\n",
    "    end = idx\n",
    "    if idx > 0:\n",
    "        start = idx-1\n",
    "    if idx < len(df):\n",
    "        end = idx+1\n",
    "    return(start, end)\n",
    "\n",
    "\n",
    "def n_upper(token, sentence):\n",
    "    ''' returns uppercase tokens surrounding term '''\n",
    "    span = ''\n",
    "    idx = sentence.split().index(token)\n",
    "    while idx > 0:\n",
    "        idx = idx - 1\n",
    "        if sentence.split()[idx][0].isupper():\n",
    "            if len(span) > 1:\n",
    "                span = sentence.split()[idx] + ' ' + span\n",
    "            else:\n",
    "                span = span + sentence.split()[idx]\n",
    "        else:\n",
    "            return span\n",
    "    return span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Candidate Passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Dam sentences: 3150\n",
      "Candidate Stream sentences: 13512\n"
     ]
    }
   ],
   "source": [
    "cand_dam = np.zeros((len(df),), dtype=int)\n",
    "cand_stream = np.zeros((len(df),), dtype=int)\n",
    "\n",
    "for idx, i in enumerate(df['docid']):\n",
    "    doc, sentid, wordidx, words, poses, ners = df.loc[idx]\n",
    "    \n",
    "    if 'Dam' in words or 'dam' in words and 'DATE' in ners:\n",
    "        cand_dam[idx] = 1\n",
    "    if 'stream' in words or 'Stream' in words or 'River' in words or 'river' in words:\n",
    "        cand_stream[idx] = 1\n",
    "\n",
    "# add to df\n",
    "df['cand_stream'] = cand_stream\n",
    "df['cand_dam'] = cand_dam\n",
    "\n",
    "print('Candidate Dam sentences: %s' %np.unique(cand_dam, return_counts=True)[1][1])\n",
    "print('Candidate Stream sentences: %s' %np.unique(cand_stream, return_counts=True)[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling functions\n",
    "\n",
    "These are based loosely on Snorkel LFs. These are intended to filter down the candidates \n",
    "```python\n",
    "\n",
    "CandidateExtractor(Dam_Removal_Year, [ngrams, ngrams], [DateMatcher(), DictionaryMatch(d=rm)])\n",
    "\n",
    "def LF_timeframe(c):\n",
    "    ''' LF to ensure the dam removal is within a timeframe'''\n",
    "    try: \n",
    "        c = int(c.year.get_span())\n",
    "        if c > 1890 and c < 2020:\n",
    "            return 1\n",
    "        else: return 0\n",
    "    except:\n",
    "        return 0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dam_extract(sent):\n",
    "    ''' Attempt to extract the name of the dam based on uppercase prior tokens '''\n",
    "    for i in sent.split():\n",
    "        if 'Dam' == i:\n",
    "            term = 'Dam'\n",
    "        if i == 'dam':\n",
    "            term = 'dam'\n",
    "    return n_upper(term, sent)\n",
    "\n",
    "\n",
    "def removal_present(series):\n",
    "    rm = ['remove', 'removal', 'breach', 'destroyed', 'destroy', 'failed', \n",
    "          'removed', 'breached', 'removing', 'post-dam', 'demolition', 'demolish',\n",
    "          'demolished', 'razing', 'razed', 'raze']\n",
    "    \n",
    "    doc, sentid, wordidx, words, poses, ners, *_ = series\n",
    "    \n",
    "    for i in rm:\n",
    "        if i in words:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def dam_nearby(sent):\n",
    "    for i in sent.split():\n",
    "        if i in dams['name'].tolist():\n",
    "            return 1 \n",
    "    return 0\n",
    "\n",
    "\n",
    "removal_present(df.iloc[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flagging Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dam Flagged: 1290 labels found\n",
      "River Flagged: 9 labels found\n",
      "Sample Dam Sentence: Dam removal is becoming an increasingly common component of river restoration -LRB- Grant , 2001 ; Pizzuto , 2002 ; Graf , 2003 -RRB- .\n",
      "Sample Stream Sentence: When signiﬁcance levels for statistical tests were provided , they are reported along with the conclusion of S or NS -LRB- signiﬁcant , non-signiﬁcant , respectively -RRB- Study Location Scale -LRB- # streams evaluated -RRB- land use Heterogeneity measure Signiﬁcance of heterogeneity effect Beisel et al. -LRB- 1998 -RRB- Harper et al. -LRB- 1997 -RRB- Minshall & Robinson -LRB- 1998 -RRB- Robson & Chester -LRB- 1999 -RRB- Buffagni et al. -LRB- 2000 -RRB- Brown -LRB- 2003 -RRB- Boyero & Bosch -LRB- 2004 -RRB- Urban et al. -LRB- 2006 -RRB- Northern France Ireland ; Czech Republic ID , U.S.A. Hobart , Tasmania North Italy NH , U.S.A. Panama CT , U.S.A. Reach and sub-reach -LRB- four streams -RRB- Forest Reach -LRB- multiple ` rivers ' -RRB- Forest Reach -LRB- 32 streams -RRB- Forest Rifﬂes -LRB- one stream -RRB- Unknown land use Reach -LRB- one river -RRB- ` unaltered with high water quality ' Rifﬂes -LRB- one stream -RRB- Forest Rifﬂes -LRB- one stream -RRB- Forest Multi-scale study : rifﬂe to watershed -LRB- 18 streams -RRB- Streams along urbanisation gradient Patch diversity and within -- patch diversity -LRB- substrate size , ﬂow , depth -RRB- Habitat functional diversity -LRB- # of patch categories -RRB- Coefﬁcient of variation in substrate size Fractal dimension within patches and number of patches within a rifﬂe -LRB- substrate size -RRB- Identiﬁed dominant habitats -LRB- units -RRB- and characterised each based on substrate , ﬂow , depth , roughness Substrate heterogeneity -LRB- size -RRB- using diversity indices Variability in substrate types -LRB- qualitatively assessed size -RRB- and velocity in rifﬂe Variation in habitat , substrate and ﬂow NS -- No difference in species richness across patches that differed in substrate or patch ` richness ' -LRB- i.e. heterogeneity -RRB- but abstract makes conﬂicting statements S -- Within-patch -LRB- bryophyte patches -RRB- complexity was associated with more species NS -- Could not distinguish between main channel -LRB- with lower habitat diversity -RRB- and ﬂoodplain regions with high habitat diversity ; attribute it to water quality differences NS -- Compared substrate Coefﬁcient of Variablity to species richness among the three groups of streams S -- P < 0.05 ; More species on boulder-cobble rifﬂes than in bedrock rifﬂes NS -- Paper focused on showing that species habitat type -LRB- unit -RRB- is not a surrogate for biota but authors also provide data on roughness , means and SD for ﬂow , depth , grain size .\n"
     ]
    }
   ],
   "source": [
    "flagged_dam = np.zeros((len(df),), dtype=int)\n",
    "flagged_stream = np.zeros((len(df),), dtype=int)\n",
    "\n",
    "# Get candidate dams \n",
    "for idx, i in df[df['cand_dam'] == 1].iterrows():\n",
    "    start, end = n_sents(idx, df)\n",
    "    \n",
    "    for sent in [start, idx, end]:\n",
    "        if removal_present(df.iloc[sent]) == 1:\n",
    "            flagged_dam[idx] = 1\n",
    "\n",
    "# Get candidate rivers\n",
    "for idx, i in df[df['cand_stream'] == 1].iterrows():\n",
    "    start, end = n_sents(idx, df)\n",
    "    \n",
    "    for sent in [start, idx, end]:\n",
    "        if dam_nearby(remap_sent(df.iloc[sent]['words'])) == 1:\n",
    "            flagged_stream[idx] = 1\n",
    "\n",
    "\n",
    "# Add to dataframe\n",
    "df['flagged_dam'] = flagged_dam\n",
    "df['flagged_stream'] = flagged_stream\n",
    "\n",
    "# Display totals\n",
    "print('Dam Flagged: %s labels found' %df[df['flagged_dam'] == 1].shape[0])\n",
    "print('River Flagged: %s labels found' %df[df['flagged_stream'] == 1].shape[0])\n",
    "\n",
    "sample_flagged_dam = df['words'][df['flagged_dam']==1].iloc[5]\n",
    "sample_flagged_stream = df['words'][df['flagged_stream']==1].iloc[5]\n",
    "print('Sample Dam Sentence: %s' %remap_sent(sample_flagged_dam))\n",
    "print('Sample Stream Sentence: %s' % remap_sent(sample_flagged_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Removal Year Extraction\n",
    "\n",
    "REDO THIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cand in [40:90]:\n",
    "    doc, sentid, wordidx, words, poses, ners, *_ = df.loc[cand]\n",
    "    if 'dam' in words or 'Dam' in words:\n",
    "        dam = dam_extract(remap_sent(df['words'].iloc[cand]))\n",
    "        try:\n",
    "            if dam != '':\n",
    "                dates = []\n",
    "                for idx, i in enumerate(ners): \n",
    "                    if i == 'DATE':\n",
    "                        dates.append(words[idx])\n",
    "                if len(dates) > 0: print(dam, dates, remap_sent(df['words'].iloc[cand]), doc)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3.6]",
   "language": "python",
   "name": "conda-env-py3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
